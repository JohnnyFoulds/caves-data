Title,Authors,DOI,DOI link,Venue,Citation count,Year,Abstract summary
Multi-Stage Pretraining for Low-Resource Domain Adaptation,"Rong Zhang, Revanth Reddy Gangi Reddy, Md Arafat Sultan, Vittorio Castelli, Anthony Ferritto, Radu Florian, Efsun Sarioglu Kayi, S. Roukos, Avirup Sil, T. Ward",10.18653/v1/2020.emnlp-main.440,https://doi.org/10.18653/v1/2020.emnlp-main.440,Conference on Empirical Methods in Natural Language Processing,34,2020,Extending the vocabulary of the language model leads to further gains.
Adjusting BERT's Pooling Layer for Large-Scale Multi-Label Text Classification,"Jan  Lehecka, Jan  Svec, Pavel  Ircing, Lubos  Smídl",10.1007/978-3-030-58323-1_23,https://doi.org/10.1007/978-3-030-58323-1_23,TDS,6,2020,A pooling layer architecture on top of BERT models improves the quality of classification by using information from the standard CLS token in combination with pooled sequence output.
Adaptive Selection of BERT Layer for Multi-Label Text Classification,"Zhichao Chen, Han Liu, Qin Zhang",10.1109/ICMLC58545.2023.10327995,https://doi.org/10.1109/ICMLC58545.2023.10327995,International Conference on Machine Learning and Computing,0,2023,The best layer that leads to the optimal performance of classification may be varied for different labels.
Feature Adaptation of Pre-Trained Language Models across Languages and Domains for Text Classification,"Hai Ye, Qingyu Tan, Ruidan He, Juntao Li, H. Ng, Lidong Bing",-,-,arXiv.org,6,2020,Class-aware feature self-distillation can consistently improve the performance of self-training in cross-domain and cross-language settings.
Multi-Label Text Classification Based on BERT and Label Attention Mechanism,"Xinghong Chen, Yi Yin, Tao Feng",10.1109/IPEC57296.2023.00073,https://doi.org/10.1109/IPEC57296.2023.00073,International Symposium on Parameterized and Exact Computation,1,2023,The proposed method had an improvement on both AAPD and RCVl-v2 datasets.
Domain Adaptation for Learning from Label Proportions Using Self-Training,"Ehsan Mohammady Ardehaly, A. Culotta",-,-,International Joint Conference on Artificial Intelligence,20,2016,A learning from label proportions classifier fit in one domain can be modified to classify instances from another domain.
Looking back at Labels: A Class based Domain Adaptation Technique,"V. Kurmi, Vinay P. Namboodiri",10.1109/IJCNN.2019.8852199,https://doi.org/10.1109/IJCNN.2019.8852199,IEEE International Joint Conference on Neural Network,29,2019,The adversarial discriminator can guide the features of the target set of classes to a more structure adapted space.
Adapting Transformers for Multi-Label Text Classification,"Haytame Fallah, P. Bellot, Emmanuel Bruno, Elisabeth Murisasco",-,-,Joint Conference of the Information Retrieval Communities in Europe,1,2022,Pre-trained language models have proven to be effective in multi-class text classification.