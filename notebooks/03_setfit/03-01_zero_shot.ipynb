{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03-01 : Zero-Shot Classification \n",
    "\n",
    "Experiment with the `get_templated_dataset` function from the `setfit` module and test the zero-shot classification capabilities of the `setfit` model. \n",
    "\n",
    "## References\n",
    "\n",
    "- [SetFit: Zero-shot Text Classification](https://huggingface.co/docs/setfit/en/how_to/zero_shot)\n",
    "- [SetFit: get_templated_dataset](https://huggingface.co/docs/setfit/v1.0.3/en/reference/utility#setfit.get_templated_dataset)\n",
    "- [Suggestions for Data Annotation with SetFit in Zero-shot Text Classification](https://github.com/huggingface/cookbook/blob/c2e1869b9608a7fa52278be5a587bcbf530383b5/notebooks/en/labelling_feedback_setfit.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 19:01:05.921408: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-20 19:01:05.921434: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-20 19:01:05.922326: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-20 19:01:05.927067: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-20 19:01:06.389443: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from setfit import get_templated_dataset\n",
    "from setfit import SetFitModel, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data'\n",
    "input_path = f'{data_path}/input/labelled_tweets/csv_labels'\n",
    "train_input_file = f'{input_path}/train.csv'\n",
    "test_input_file = f'{input_path}/test.csv'\n",
    "val_input_file = f'{input_path}/val.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_input_file)\n",
    "df_val = pd.read_csv(val_input_file)\n",
    "df_test = pd.read_csv(test_input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Labels to List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['labels_list'] = df_train['labels'].str.split(' ')\n",
    "df_test['labels_list'] = df_test['labels'].str.split(' ')\n",
    "df_val['labels_list'] = df_val['labels'].str.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Multi-label Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 12\n",
      "Vocabulary: ['conspiracy' 'country' 'ineffective' 'ingredients' 'mandatory' 'none'\n",
      " 'pharma' 'political' 'religious' 'rushed' 'side-effect' 'unnecessary']\n"
     ]
    }
   ],
   "source": [
    "# get the list of label values\n",
    "labels = pd.concat([df_train.labels_list, \n",
    "                    df_val.labels_list, \n",
    "                    df_test.labels_list])\n",
    "\n",
    "# initialize MultiLabelBinarizer\n",
    "labels_lookup = MultiLabelBinarizer()\n",
    "\n",
    "# learn the vocabulary\n",
    "labels_lookup = labels_lookup.fit(labels)\n",
    "\n",
    "# show the vocabulary\n",
    "vocab = labels_lookup.classes_\n",
    "print(f'Vocabulary size: {len(vocab)}')\n",
    "print(f'Vocabulary: {vocab}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the data frame with a `labels_encoded` column\n",
    "df_train['labels_encoded'] = labels_lookup.transform(df_train.labels_list).tolist()\n",
    "df_val['labels_encoded'] = labels_lookup.transform(df_val.labels_list).tolist()\n",
    "df_test['labels_encoded'] = labels_lookup.transform(df_test.labels_list).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the one-hot encoded labels as columns to the data frames\n",
    "df_train = df_train.join(pd.DataFrame(labels_lookup.transform(df_train.labels_list), \n",
    "                                     columns=labels_lookup.classes_, \n",
    "                                     index=df_train.index))\n",
    "\n",
    "df_val = df_val.join(pd.DataFrame(labels_lookup.transform(df_val.labels_list),\n",
    "                                    columns=labels_lookup.classes_,\n",
    "                                    index=df_val.index))\n",
    "\n",
    "df_test = df_test.join(pd.DataFrame(labels_lookup.transform(df_test.labels_list),\n",
    "                                    columns=labels_lookup.classes_,\n",
    "                                    index=df_test.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test get_templated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels = vocab.tolist()\n",
    "template='This vaccine concern is about {}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['This vaccine concern is about conspiracy',\n",
       "  'This vaccine concern is about conspiracy',\n",
       "  'This vaccine concern is about conspiracy',\n",
       "  'This vaccine concern is about conspiracy',\n",
       "  'This vaccine concern is about conspiracy',\n",
       "  'This vaccine concern is about conspiracy',\n",
       "  'This vaccine concern is about conspiracy',\n",
       "  'This vaccine concern is about conspiracy',\n",
       "  'This vaccine concern is about country',\n",
       "  'This vaccine concern is about country',\n",
       "  'This vaccine concern is about country',\n",
       "  'This vaccine concern is about country',\n",
       "  'This vaccine concern is about country',\n",
       "  'This vaccine concern is about country',\n",
       "  'This vaccine concern is about country',\n",
       "  'This vaccine concern is about country',\n",
       "  'This vaccine concern is about ineffective',\n",
       "  'This vaccine concern is about ineffective',\n",
       "  'This vaccine concern is about ineffective',\n",
       "  'This vaccine concern is about ineffective',\n",
       "  'This vaccine concern is about ineffective',\n",
       "  'This vaccine concern is about ineffective',\n",
       "  'This vaccine concern is about ineffective',\n",
       "  'This vaccine concern is about ineffective',\n",
       "  'This vaccine concern is about ingredients',\n",
       "  'This vaccine concern is about ingredients',\n",
       "  'This vaccine concern is about ingredients',\n",
       "  'This vaccine concern is about ingredients',\n",
       "  'This vaccine concern is about ingredients',\n",
       "  'This vaccine concern is about ingredients',\n",
       "  'This vaccine concern is about ingredients',\n",
       "  'This vaccine concern is about ingredients',\n",
       "  'This vaccine concern is about mandatory',\n",
       "  'This vaccine concern is about mandatory',\n",
       "  'This vaccine concern is about mandatory',\n",
       "  'This vaccine concern is about mandatory',\n",
       "  'This vaccine concern is about mandatory',\n",
       "  'This vaccine concern is about mandatory',\n",
       "  'This vaccine concern is about mandatory',\n",
       "  'This vaccine concern is about mandatory',\n",
       "  'This vaccine concern is about none',\n",
       "  'This vaccine concern is about none',\n",
       "  'This vaccine concern is about none',\n",
       "  'This vaccine concern is about none',\n",
       "  'This vaccine concern is about none',\n",
       "  'This vaccine concern is about none',\n",
       "  'This vaccine concern is about none',\n",
       "  'This vaccine concern is about none',\n",
       "  'This vaccine concern is about pharma',\n",
       "  'This vaccine concern is about pharma',\n",
       "  'This vaccine concern is about pharma',\n",
       "  'This vaccine concern is about pharma',\n",
       "  'This vaccine concern is about pharma',\n",
       "  'This vaccine concern is about pharma',\n",
       "  'This vaccine concern is about pharma',\n",
       "  'This vaccine concern is about pharma',\n",
       "  'This vaccine concern is about political',\n",
       "  'This vaccine concern is about political',\n",
       "  'This vaccine concern is about political',\n",
       "  'This vaccine concern is about political',\n",
       "  'This vaccine concern is about political',\n",
       "  'This vaccine concern is about political',\n",
       "  'This vaccine concern is about political',\n",
       "  'This vaccine concern is about political',\n",
       "  'This vaccine concern is about religious',\n",
       "  'This vaccine concern is about religious',\n",
       "  'This vaccine concern is about religious',\n",
       "  'This vaccine concern is about religious',\n",
       "  'This vaccine concern is about religious',\n",
       "  'This vaccine concern is about religious',\n",
       "  'This vaccine concern is about religious',\n",
       "  'This vaccine concern is about religious',\n",
       "  'This vaccine concern is about rushed',\n",
       "  'This vaccine concern is about rushed',\n",
       "  'This vaccine concern is about rushed',\n",
       "  'This vaccine concern is about rushed',\n",
       "  'This vaccine concern is about rushed',\n",
       "  'This vaccine concern is about rushed',\n",
       "  'This vaccine concern is about rushed',\n",
       "  'This vaccine concern is about rushed',\n",
       "  'This vaccine concern is about side-effect',\n",
       "  'This vaccine concern is about side-effect',\n",
       "  'This vaccine concern is about side-effect',\n",
       "  'This vaccine concern is about side-effect',\n",
       "  'This vaccine concern is about side-effect',\n",
       "  'This vaccine concern is about side-effect',\n",
       "  'This vaccine concern is about side-effect',\n",
       "  'This vaccine concern is about side-effect',\n",
       "  'This vaccine concern is about unnecessary',\n",
       "  'This vaccine concern is about unnecessary',\n",
       "  'This vaccine concern is about unnecessary',\n",
       "  'This vaccine concern is about unnecessary',\n",
       "  'This vaccine concern is about unnecessary',\n",
       "  'This vaccine concern is about unnecessary',\n",
       "  'This vaccine concern is about unnecessary',\n",
       "  'This vaccine concern is about unnecessary'],\n",
       " 'label': [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = get_templated_dataset(\n",
    "    candidate_labels=candidate_labels,\n",
    "    sample_size=8,\n",
    "    template=template,\n",
    "    multi_label=True)\n",
    "\n",
    "test_dataset.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'BAAI/bge-small-en-v1.5'\n",
    "model_name = 'sentence-transformers/paraphrase-mpnet-base-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(candidate_labels, template, multi_label=False, model_name=\"all-MiniLM-L6-v2\"):\n",
    "    # build a training dataset to train the zero-shot classifier\n",
    "    train_dataset = get_templated_dataset(\n",
    "        candidate_labels=candidate_labels,\n",
    "        sample_size=8,\n",
    "        template=template,\n",
    "        multi_label=multi_label\n",
    "    )\n",
    "\n",
    "    # train a model using the training dataset we just built\n",
    "    if multi_label:\n",
    "        model = SetFitModel.from_pretrained(\n",
    "            model_name,\n",
    "            multi_target_strategy=\"one-vs-rest\"\n",
    "        )\n",
    "    else:\n",
    "        model = SetFitModel.from_pretrained(\n",
    "            model_name\n",
    "        )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset\n",
    "    )\n",
    "    trainer.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Train a multi-label classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnny/swan/miniconda3/envs/caves-data/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "/home/johnny/swan/miniconda3/envs/caves-data/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "/home/johnny/swan/miniconda3/envs/caves-data/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  ) < LooseVersion(\"1.15\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16222ebeef3d4c1d8d64b23088b0d89c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/96 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 8448\n",
      "  Batch size = 16\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 528\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='528' max='528' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [528/528 00:24, Epoch 1/0]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the classification model\n",
    "model = train_model(\n",
    "    candidate_labels=candidate_labels, \n",
    "    template=template, \n",
    "    multi_label=True,\n",
    "    model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(texts, model, labels):\n",
    "    probas = model.predict_proba(texts, as_numpy=True)\n",
    "    return probas.tolist()\n",
    "    # for pred in probas:\n",
    "    #     yield [{\"label\": label, \"score\": score} for label, score in zip(labels, pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.058900424129974226,\n",
       "  0.006439022990380861,\n",
       "  0.005081788219557832,\n",
       "  0.012280360696443077,\n",
       "  0.007821252784898222,\n",
       "  0.006176522814892339,\n",
       "  0.08039143365121408,\n",
       "  0.01338885843353766,\n",
       "  0.005539749772679079,\n",
       "  0.006989365918959436,\n",
       "  0.009635772295716006,\n",
       "  0.010520266009283172],\n",
       " [0.024651297876294116,\n",
       "  0.009563608843233254,\n",
       "  0.044754771974295886,\n",
       "  0.006569937650931341,\n",
       "  0.01788917737873444,\n",
       "  0.007851320421729029,\n",
       "  0.0062969367190919626,\n",
       "  0.009027307022265549,\n",
       "  0.008291270815444028,\n",
       "  0.008526647686099297,\n",
       "  0.007935537992653821,\n",
       "  0.016064096455197113],\n",
       " [0.004783939401488553,\n",
       "  0.007850681968734891,\n",
       "  0.04473901070064493,\n",
       "  0.021152890471366106,\n",
       "  0.00610784862872318,\n",
       "  0.007830827931005004,\n",
       "  0.013756584165896271,\n",
       "  0.005118114310428941,\n",
       "  0.010434421524870487,\n",
       "  0.009960489671893582,\n",
       "  0.056581654897407845,\n",
       "  0.006821455108851918],\n",
       " [0.006787426250942632,\n",
       "  0.007546201426519945,\n",
       "  0.014644516460107167,\n",
       "  0.005360994440365425,\n",
       "  0.01020591377211537,\n",
       "  0.00502485220637137,\n",
       "  0.009101547663059595,\n",
       "  0.005467835660861391,\n",
       "  0.005214794057393589,\n",
       "  0.44464626688071063,\n",
       "  0.01045909125703198,\n",
       "  0.012004488912315087],\n",
       " [0.01133774397628535,\n",
       "  0.005640467116396343,\n",
       "  0.06605566445795626,\n",
       "  0.007248735409032736,\n",
       "  0.007377969102004988,\n",
       "  0.036375530447809896,\n",
       "  0.006776078345541151,\n",
       "  0.0053026630318853685,\n",
       "  0.010652067928313701,\n",
       "  0.013090402448504457,\n",
       "  0.01733532322695314,\n",
       "  0.006423024396790501]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = list(get_predictions(data.text.values, model, candidate_labels))\n",
    "y_pred[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caves-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
