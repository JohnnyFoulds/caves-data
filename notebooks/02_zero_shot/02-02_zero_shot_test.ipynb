{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02-02 : Zero Shot on Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 08:36:55.506454: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-20 08:36:55.506482: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-20 08:36:55.507390: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-20 08:36:55.511542: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-20 08:36:56.046486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from typing import Dict, List\n",
    "from pprint import pprint\n",
    "from pqdm.threads import pqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, jaccard_score, accuracy_score, f1_score\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data'\n",
    "input_path = f'{data_path}/input/labelled_tweets/csv_labels'\n",
    "train_input_file = f'{input_path}/train.csv'\n",
    "test_input_file = f'{input_path}/test.csv'\n",
    "val_input_file = f'{input_path}/val.csv'\n",
    "output_path = f'{data_path}/output/02_zero_shot'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (6957, 3)\n",
      "Val shape: (987, 3)\n",
      "Test shape: (1977, 3)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(train_input_file)\n",
    "df_val = pd.read_csv(val_input_file)\n",
    "df_test = pd.read_csv(test_input_file)\n",
    "\n",
    "# show the data frame shapes\n",
    "print(f'Train shape: {df_train.shape}')\n",
    "print(f'Val shape: {df_val.shape}')\n",
    "print(f'Test shape: {df_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Labels to List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['labels_list'] = df_train['labels'].str.split(' ')\n",
    "df_test['labels_list'] = df_test['labels'].str.split(' ')\n",
    "df_val['labels_list'] = df_val['labels'].str.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Multi-label Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 12\n",
      "Vocabulary: ['conspiracy' 'country' 'ineffective' 'ingredients' 'mandatory' 'none'\n",
      " 'pharma' 'political' 'religious' 'rushed' 'side-effect' 'unnecessary']\n"
     ]
    }
   ],
   "source": [
    "# get the list of label values\n",
    "labels = pd.concat([df_train.labels_list, \n",
    "                    df_val.labels_list, \n",
    "                    df_test.labels_list])\n",
    "\n",
    "# initialize MultiLabelBinarizer\n",
    "labels_lookup = MultiLabelBinarizer()\n",
    "\n",
    "# learn the vocabulary\n",
    "labels_lookup = labels_lookup.fit(labels)\n",
    "\n",
    "# show the vocabulary\n",
    "vocab = labels_lookup.classes_\n",
    "print(f'Vocabulary size: {len(vocab)}')\n",
    "print(f'Vocabulary: {vocab}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the data frame with a `labels_encoded` column\n",
    "df_train['labels_encoded'] = labels_lookup.transform(df_train.labels_list).tolist()\n",
    "df_val['labels_encoded'] = labels_lookup.transform(df_val.labels_list).tolist()\n",
    "df_test['labels_encoded'] = labels_lookup.transform(df_test.labels_list).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the one-hot encoded labels as columns to the data frames\n",
    "df_train = df_train.join(pd.DataFrame(labels_lookup.transform(df_train.labels_list), \n",
    "                                     columns=labels_lookup.classes_, \n",
    "                                     index=df_train.index))\n",
    "\n",
    "df_val = df_val.join(pd.DataFrame(labels_lookup.transform(df_val.labels_list),\n",
    "                                    columns=labels_lookup.classes_,\n",
    "                                    index=df_val.index))\n",
    "\n",
    "df_test = df_test.join(pd.DataFrame(labels_lookup.transform(df_test.labels_list),\n",
    "                                    columns=labels_lookup.classes_,\n",
    "                                    index=df_test.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Create Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 08:36:58.367765: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-20 08:36:58.396199: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-20 08:36:58.396406: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-20 08:36:58.397544: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-20 08:36:58.397719: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-20 08:36:58.397866: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-20 08:36:58.454334: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-20 08:36:58.454494: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-20 08:36:58.454640: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-20 08:36:58.454747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22110 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:08:00.0, compute capability: 8.6\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBartForSequenceClassification: ['model.decoder.version', 'model.encoder.version']\n",
      "- This IS expected if you are initializing TFBartForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBartForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# the model that will be used for classification\n",
    "model_name = 'facebook/bart-large-mnli'\n",
    "\n",
    "# create the classifier\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Get Standardized Predictions\n",
    "\n",
    "Standardize the prediction to match the order of the labels in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_prediction(prediction: Dict, vocabulary:List[str]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Standardize the prediction output to a fixed length list.\n",
    "    \"\"\"\n",
    "    return [prediction['scores'][prediction['labels'].index(label)]\n",
    "            for label in vocabulary]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(text:str, classifier, vocabulary:List[str]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Get the prediction for a given text.\n",
    "    \"\"\"\n",
    "    result = classifier(\n",
    "        sequences=text,\n",
    "        candidate_labels=vocabulary,\n",
    "        hypothesis_template='This concern with the vaccine is about {}.',\n",
    "        multi_label=True)\n",
    "    \n",
    "    return standardize_prediction(result, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: List[str], vocabulary:List[str], classifier, n_jobs:int=1) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Predict the labels for a list of texts.\n",
    "    \"\"\"\n",
    "    if n_jobs == 1:\n",
    "        result = []\n",
    "        for text in tqdm(X):\n",
    "            result.append(get_prediction(text, classifier, vocabulary))\n",
    "            \n",
    "        return result\n",
    "    else:\n",
    "        # create the partial function for parallel processing\n",
    "        get_prediction_partial = partial(get_prediction, classifier=classifier, vocabulary=vocabulary)\n",
    "    \n",
    "        # perform parallel processing \n",
    "        return pqdm(X, get_prediction_partial, n_jobs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_score_macro(y_true, y_pred):\n",
    "        \"\"\"Calculate F1-score (Macro-Average).\"\"\"\n",
    "        return f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_score_weighted(y_true, y_pred):\n",
    "        \"\"\"Calculate F1-score (Weighted-Average).\"\"\"\n",
    "        return f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def jaccard_similarity(y_true, y_pred):\n",
    "        \"\"\"Calculate average Jaccard Similarity.\"\"\"\n",
    "        return jaccard_score(y_true, y_pred, average='samples')\n",
    "\n",
    "    @staticmethod\n",
    "    def subset_accuracy(y_true, y_pred):\n",
    "        \"\"\"Calculate Subset Accuracy (Exact Match Accuracy).\"\"\"\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_all(y_true,\n",
    "                     y_pred,\n",
    "                     threshold:float=0.5):\n",
    "        \n",
    "        # Convert predictions to binary\n",
    "        y_pred_bin = [[int(prob > threshold) for prob in pred] for pred in y_pred]\n",
    "        \n",
    "        \"\"\"Evaluate all metrics and display a summary.\"\"\"\n",
    "        f1_macro = Evaluation.f1_score_macro(y_true, y_pred_bin)\n",
    "        f1_weighted = Evaluation.f1_score_weighted(y_true, y_pred_bin)\n",
    "        jaccard_similarity = Evaluation.jaccard_similarity(y_true, y_pred_bin)\n",
    "        subset_accuracy = Evaluation.subset_accuracy(y_true, y_pred_bin)\n",
    "\n",
    "        # Display a summary of the evaluation\n",
    "        print(f\"F1 Score (Macro-Average)   \\t{f1_macro:.3f}\")\n",
    "        print(f\"F1 Score (Weighted-Average)\\t{f1_weighted:.3f}\")\n",
    "        print(f\"Average Jaccard Similarity \\t{jaccard_similarity:.3f}\")\n",
    "        print(f\"Subset Accuracy            \\t{subset_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Perform Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b16b99ad4ba4345926e91e8e9b92e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/1977 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf60965c5604084884b7be57a49dc68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/1977 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = df_test\n",
    "\n",
    "# get the true values\n",
    "y_true = data[vocab].values.tolist()\n",
    "\n",
    "# get the predictions\n",
    "y_pred = predict(X=data.text.tolist(),\n",
    "        vocabulary=vocab.tolist(),\n",
    "        classifier=classifier,\n",
    "        n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save y_true to a file\n",
    "np.save(f'{output_path}/02-01_bart-large-mnli_test_true.npy', y_true)\n",
    "\n",
    "# save the predictions to a file\n",
    "np.save(f'{output_path}/02-01_bart-large-mnli_test.npy', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_classification_report(data:pd.DataFrame,\n",
    "                               y_pred:np.ndarray,\n",
    "                               threshold:float=0.5):\n",
    "    # get the true labels\n",
    "    y_true = data[vocab].values\n",
    "    \n",
    "    # Convert predictions to binary\n",
    "    y_pred_bin = [[int(prob > threshold) for prob in pred] for pred in y_pred]\n",
    "    \n",
    "    # show the classification report\n",
    "    print(classification_report(y_true, y_pred_bin, target_names=vocab))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the test classification report\n",
    "show_classification_report(data, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Full Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation.evaluate_all(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caves-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
