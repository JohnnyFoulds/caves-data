{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02-01 : Zero Shot Text Classification\n",
    "\n",
    "## References\n",
    "\n",
    "- [Unlocking Zero-Shot Text Classification with Hugging Faceâ€™s Transformers](https://medium.com/@s.sadathosseini/unlocking-zero-shot-text-classification-with-hugging-faces-transformers-9e30de5c8455)\n",
    "- [Aspect Mining Using Zero-Shot Classification](https://aiswaryaramachandran.medium.com/aspect-mining-using-zero-shot-classification-3190e8a89d68)\n",
    "- [Exploring Hugging Face: Zero-Shot Classification](https://pub.aimind.so/exploring-hugging-face-zero-shot-classification-781ef3a18510)\n",
    "- [Zero Shot Classification with Huggingface ðŸ¤— + Sentence Transformers](https://sachin-abeywardana.medium.com/zero-shot-classification-with-huggingface-sentence-transformers-c6cd732de0e0)\n",
    "- [Analyzing QAnon on Twitter with Zero-Shot Classification](https://towardsdatascience.com/analyzing-qanon-on-twitter-with-zero-shot-classification-13ad73d324fc)\n",
    "- [MoritzLaurer/deberta-v3-large-zeroshot-v2.0](https://huggingface.co/MoritzLaurer/deberta-v3-large-zeroshot-v2.0)\n",
    "- [facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)\n",
    "\n",
    " ### Interesting Models\n",
    "\n",
    "- [FacebookAI/roberta-large-mnli](https://huggingface.co/FacebookAI/roberta-large-mnli) - fine-tuned on the Multi-Genre Natural Language Inference (MNLI) corpus.\n",
    "- [MoritzLaurer/deberta-v3-large-zeroshot-v2.0](https://huggingface.co/MoritzLaurer/deberta-v3-large-zeroshot-v2.0)\n",
    "- [facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from typing import Dict, List\n",
    "from pprint import pprint\n",
    "from pqdm.threads import pqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, jaccard_score, accuracy_score, f1_score\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data'\n",
    "input_path = f'{data_path}/input/labelled_tweets/csv_labels'\n",
    "train_input_file = f'{input_path}/train.csv'\n",
    "test_input_file = f'{input_path}/test.csv'\n",
    "val_input_file = f'{input_path}/val.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_input_file)\n",
    "df_val = pd.read_csv(val_input_file)\n",
    "df_test = pd.read_csv(test_input_file)\n",
    "\n",
    "# show the data frame shapes\n",
    "print(f'Train shape: {df_train.shape}')\n",
    "print(f'Val shape: {df_val.shape}')\n",
    "print(f'Test shape: {df_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Labels to List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['labels_list'] = df_train['labels'].str.split(' ')\n",
    "df_test['labels_list'] = df_test['labels'].str.split(' ')\n",
    "df_val['labels_list'] = df_val['labels'].str.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Multi-label Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of label values\n",
    "labels = pd.concat([df_train.labels_list, \n",
    "                    df_val.labels_list, \n",
    "                    df_test.labels_list])\n",
    "\n",
    "# initialize MultiLabelBinarizer\n",
    "labels_lookup = MultiLabelBinarizer()\n",
    "\n",
    "# learn the vocabulary\n",
    "labels_lookup = labels_lookup.fit(labels)\n",
    "\n",
    "# show the vocabulary\n",
    "vocab = labels_lookup.classes_\n",
    "print(f'Vocabulary size: {len(vocab)}')\n",
    "print(f'Vocabulary: {vocab}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the data frame with a `labels_encoded` column\n",
    "df_train['labels_encoded'] = labels_lookup.transform(df_train.labels_list).tolist()\n",
    "df_val['labels_encoded'] = labels_lookup.transform(df_val.labels_list).tolist()\n",
    "df_test['labels_encoded'] = labels_lookup.transform(df_test.labels_list).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the one-hot encoded labels as columns to the data frames\n",
    "df_train = df_train.join(pd.DataFrame(labels_lookup.transform(df_train.labels_list), \n",
    "                                     columns=labels_lookup.classes_, \n",
    "                                     index=df_train.index))\n",
    "\n",
    "df_val = df_val.join(pd.DataFrame(labels_lookup.transform(df_val.labels_list),\n",
    "                                    columns=labels_lookup.classes_,\n",
    "                                    index=df_val.index))\n",
    "\n",
    "df_test = df_test.join(pd.DataFrame(labels_lookup.transform(df_test.labels_list),\n",
    "                                    columns=labels_lookup.classes_,\n",
    "                                    index=df_test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Create Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model that will be used for classification\n",
    "model_name = 'facebook/bart-large-mnli'\n",
    "\n",
    "# create the classifier\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Test Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a row for testing\n",
    "sample_row = df_train.iloc[146][['text', 'labels_list', 'labels_encoded']]\n",
    "pprint(sample_row.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform classification\n",
    "result = classifier(\n",
    "    sequences=sample_row.text,\n",
    "    candidate_labels=vocab,\n",
    "    hypothesis_template='This concern with the vaccine is about {}.',\n",
    "    multi_label=True)\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Get Standardized Predictions\n",
    "\n",
    "Standardize the prediction to match the order of the labels in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_prediction(prediction: Dict, vocabulary:List[str]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Standardize the prediction output to a fixed length list.\n",
    "    \"\"\"\n",
    "    return [prediction['scores'][prediction['labels'].index(label)]\n",
    "            for label in vocabulary]\n",
    "\n",
    "## test the function\n",
    "#standardize_prediction(result, vocab.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(text:str, classifier, vocabulary:List[str]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Get the prediction for a given text.\n",
    "    \"\"\"\n",
    "    result = classifier(\n",
    "        sequences=text,\n",
    "        candidate_labels=vocabulary,\n",
    "        hypothesis_template='This concern with the vaccine is about {}.',\n",
    "        multi_label=True)\n",
    "    \n",
    "    return standardize_prediction(result, vocabulary)\n",
    "\n",
    "## test the function\n",
    "#get_prediction(sample_row.text, classifier, vocab.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: List[str], vocabulary:List[str], classifier, n_jobs:int=1) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Predict the labels for a list of texts.\n",
    "    \"\"\"\n",
    "    if n_jobs == 1:\n",
    "        result = []\n",
    "        for text in tqdm(X):\n",
    "            result.append(get_prediction(text, classifier, vocabulary))\n",
    "            \n",
    "        return result\n",
    "    else:\n",
    "        # create the partial function for parallel processing\n",
    "        get_prediction_partial = partial(get_prediction, classifier=classifier, vocabulary=vocabulary)\n",
    "    \n",
    "        # perform parallel processing \n",
    "        return pqdm(X, get_prediction_partial, n_jobs=5)\n",
    "        \n",
    "## test the function\n",
    "# predict(\n",
    "#     X=df_train[:5].text.tolist(), \n",
    "#     vocabulary=vocab.tolist(),\n",
    "#     classifier=classifier,\n",
    "#     n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_score_macro(y_true, y_pred):\n",
    "        \"\"\"Calculate F1-score (Macro-Average).\"\"\"\n",
    "        return f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_score_weighted(y_true, y_pred):\n",
    "        \"\"\"Calculate F1-score (Weighted-Average).\"\"\"\n",
    "        return f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def jaccard_similarity(y_true, y_pred):\n",
    "        \"\"\"Calculate average Jaccard Similarity.\"\"\"\n",
    "        return jaccard_score(y_true, y_pred, average='samples')\n",
    "\n",
    "    @staticmethod\n",
    "    def subset_accuracy(y_true, y_pred):\n",
    "        \"\"\"Calculate Subset Accuracy (Exact Match Accuracy).\"\"\"\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_all(y_true,\n",
    "                     y_pred,\n",
    "                     threshold:float=0.5):\n",
    "        \n",
    "        # Convert predictions to binary\n",
    "        y_pred_bin = [[int(prob > threshold) for prob in pred] for pred in y_pred]\n",
    "        \n",
    "        \"\"\"Evaluate all metrics and display a summary.\"\"\"\n",
    "        f1_macro = Evaluation.f1_score_macro(y_true, y_pred_bin)\n",
    "        f1_weighted = Evaluation.f1_score_weighted(y_true, y_pred_bin)\n",
    "        jaccard_similarity = Evaluation.jaccard_similarity(y_true, y_pred_bin)\n",
    "        subset_accuracy = Evaluation.subset_accuracy(y_true, y_pred_bin)\n",
    "\n",
    "        # Display a summary of the evaluation\n",
    "        print(f\"F1 Score (Macro-Average)   \\t{f1_macro:.3f}\")\n",
    "        print(f\"F1 Score (Weighted-Average)\\t{f1_weighted:.3f}\")\n",
    "        print(f\"Average Jaccard Similarity \\t{jaccard_similarity:.3f}\")\n",
    "        print(f\"Subset Accuracy            \\t{subset_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Perform Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_train\n",
    "\n",
    "# get the true values\n",
    "y_true = data[vocab].values.tolist()\n",
    "\n",
    "# get the predictions\n",
    "y_pred = predict(X=data.text.tolist(),\n",
    "        vocabulary=vocab.tolist(),\n",
    "        classifier=classifier,\n",
    "        n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_classification_report(data:pd.DataFrame,\n",
    "                               y_pred:np.ndarray,\n",
    "                               threshold:float=0.5):\n",
    "    # get the true labels\n",
    "    y_true = data[vocab].values\n",
    "    \n",
    "    # Convert predictions to binary\n",
    "    y_pred_bin = [[int(prob > threshold) for prob in pred] for pred in y_pred]\n",
    "    \n",
    "    # show the classification report\n",
    "    print(classification_report(y_true, y_pred_bin, target_names=vocab))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the test classification report\n",
    "show_classification_report(data, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Full Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation.evaluate_all(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caves-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
