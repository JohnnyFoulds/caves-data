{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02-01 : Zero Shot Text Classification\n",
    "\n",
    "## References\n",
    "\n",
    "- [Unlocking Zero-Shot Text Classification with Hugging Faceâ€™s Transformers](https://medium.com/@s.sadathosseini/unlocking-zero-shot-text-classification-with-hugging-faces-transformers-9e30de5c8455)\n",
    "- [Aspect Mining Using Zero-Shot Classification](https://aiswaryaramachandran.medium.com/aspect-mining-using-zero-shot-classification-3190e8a89d68)\n",
    "- [Exploring Hugging Face: Zero-Shot Classification](https://pub.aimind.so/exploring-hugging-face-zero-shot-classification-781ef3a18510)\n",
    "- [Zero Shot Classification with Huggingface ðŸ¤— + Sentence Transformers](https://sachin-abeywardana.medium.com/zero-shot-classification-with-huggingface-sentence-transformers-c6cd732de0e0)\n",
    "- [Analyzing QAnon on Twitter with Zero-Shot Classification](https://towardsdatascience.com/analyzing-qanon-on-twitter-with-zero-shot-classification-13ad73d324fc)\n",
    "- [MoritzLaurer/deberta-v3-large-zeroshot-v2.0](https://huggingface.co/MoritzLaurer/deberta-v3-large-zeroshot-v2.0)\n",
    "- [facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)\n",
    "\n",
    " ### Interesting Models\n",
    "\n",
    "- [FacebookAI/roberta-large-mnli](https://huggingface.co/FacebookAI/roberta-large-mnli) - fine-tuned on the Multi-Genre Natural Language Inference (MNLI) corpus.\n",
    "- [MoritzLaurer/deberta-v3-large-zeroshot-v2.0](https://huggingface.co/MoritzLaurer/deberta-v3-large-zeroshot-v2.0)\n",
    "- [facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 19:58:54.556818: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-19 19:58:54.556845: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-19 19:58:54.557738: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-19 19:58:54.562425: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-19 19:58:55.102932: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from typing import Dict, List\n",
    "from pprint import pprint\n",
    "from pqdm.threads import pqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, jaccard_score, accuracy_score, f1_score\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data'\n",
    "input_path = f'{data_path}/input/labelled_tweets/csv_labels'\n",
    "train_input_file = f'{input_path}/train.csv'\n",
    "test_input_file = f'{input_path}/test.csv'\n",
    "val_input_file = f'{input_path}/val.csv'\n",
    "output_path = f'{data_path}/output/02_zero_shot'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (6957, 3)\n",
      "Val shape: (987, 3)\n",
      "Test shape: (1977, 3)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(train_input_file)\n",
    "df_val = pd.read_csv(val_input_file)\n",
    "df_test = pd.read_csv(test_input_file)\n",
    "\n",
    "# show the data frame shapes\n",
    "print(f'Train shape: {df_train.shape}')\n",
    "print(f'Val shape: {df_val.shape}')\n",
    "print(f'Test shape: {df_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1311981051720409089t</td>\n",
       "      <td>@sandraburgess3 They have no idea , they cant ...</td>\n",
       "      <td>ineffective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1361403925845401601t</td>\n",
       "      <td>@stepheniscowboy Nvm I â€™ ve had covid I â€™ ve g...</td>\n",
       "      <td>unnecessary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1293488278361055233t</td>\n",
       "      <td>Coronavirus updates : Government partners with...</td>\n",
       "      <td>pharma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1305252218526990338t</td>\n",
       "      <td>@OANN U . K . Glaxo Smith Klein whistleblower ...</td>\n",
       "      <td>rushed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1376135683400687618t</td>\n",
       "      <td>3 / horse \" AstraZeneca , not so much for the ...</td>\n",
       "      <td>ineffective pharma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID                                               text  \\\n",
       "0  1311981051720409089t  @sandraburgess3 They have no idea , they cant ...   \n",
       "1  1361403925845401601t  @stepheniscowboy Nvm I â€™ ve had covid I â€™ ve g...   \n",
       "2  1293488278361055233t  Coronavirus updates : Government partners with...   \n",
       "3  1305252218526990338t  @OANN U . K . Glaxo Smith Klein whistleblower ...   \n",
       "4  1376135683400687618t  3 / horse \" AstraZeneca , not so much for the ...   \n",
       "\n",
       "               labels  \n",
       "0         ineffective  \n",
       "1         unnecessary  \n",
       "2              pharma  \n",
       "3              rushed  \n",
       "4  ineffective pharma  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Labels to List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['labels_list'] = df_train['labels'].str.split(' ')\n",
    "df_test['labels_list'] = df_test['labels'].str.split(' ')\n",
    "df_val['labels_list'] = df_val['labels'].str.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Multi-label Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 12\n",
      "Vocabulary: ['conspiracy' 'country' 'ineffective' 'ingredients' 'mandatory' 'none'\n",
      " 'pharma' 'political' 'religious' 'rushed' 'side-effect' 'unnecessary']\n"
     ]
    }
   ],
   "source": [
    "# get the list of label values\n",
    "labels = pd.concat([df_train.labels_list, \n",
    "                    df_val.labels_list, \n",
    "                    df_test.labels_list])\n",
    "\n",
    "# initialize MultiLabelBinarizer\n",
    "labels_lookup = MultiLabelBinarizer()\n",
    "\n",
    "# learn the vocabulary\n",
    "labels_lookup = labels_lookup.fit(labels)\n",
    "\n",
    "# show the vocabulary\n",
    "vocab = labels_lookup.classes_\n",
    "print(f'Vocabulary size: {len(vocab)}')\n",
    "print(f'Vocabulary: {vocab}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the data frame with a `labels_encoded` column\n",
    "df_train['labels_encoded'] = labels_lookup.transform(df_train.labels_list).tolist()\n",
    "df_val['labels_encoded'] = labels_lookup.transform(df_val.labels_list).tolist()\n",
    "df_test['labels_encoded'] = labels_lookup.transform(df_test.labels_list).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the one-hot encoded labels as columns to the data frames\n",
    "df_train = df_train.join(pd.DataFrame(labels_lookup.transform(df_train.labels_list), \n",
    "                                     columns=labels_lookup.classes_, \n",
    "                                     index=df_train.index))\n",
    "\n",
    "df_val = df_val.join(pd.DataFrame(labels_lookup.transform(df_val.labels_list),\n",
    "                                    columns=labels_lookup.classes_,\n",
    "                                    index=df_val.index))\n",
    "\n",
    "df_test = df_test.join(pd.DataFrame(labels_lookup.transform(df_test.labels_list),\n",
    "                                    columns=labels_lookup.classes_,\n",
    "                                    index=df_test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>labels_list</th>\n",
       "      <th>labels_encoded</th>\n",
       "      <th>conspiracy</th>\n",
       "      <th>country</th>\n",
       "      <th>ineffective</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>mandatory</th>\n",
       "      <th>none</th>\n",
       "      <th>pharma</th>\n",
       "      <th>political</th>\n",
       "      <th>religious</th>\n",
       "      <th>rushed</th>\n",
       "      <th>side-effect</th>\n",
       "      <th>unnecessary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1311981051720409089t</td>\n",
       "      <td>@sandraburgess3 They have no idea , they cant ...</td>\n",
       "      <td>ineffective</td>\n",
       "      <td>[ineffective]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1361403925845401601t</td>\n",
       "      <td>@stepheniscowboy Nvm I â€™ ve had covid I â€™ ve g...</td>\n",
       "      <td>unnecessary</td>\n",
       "      <td>[unnecessary]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1293488278361055233t</td>\n",
       "      <td>Coronavirus updates : Government partners with...</td>\n",
       "      <td>pharma</td>\n",
       "      <td>[pharma]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1305252218526990338t</td>\n",
       "      <td>@OANN U . K . Glaxo Smith Klein whistleblower ...</td>\n",
       "      <td>rushed</td>\n",
       "      <td>[rushed]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1376135683400687618t</td>\n",
       "      <td>3 / horse \" AstraZeneca , not so much for the ...</td>\n",
       "      <td>ineffective pharma</td>\n",
       "      <td>[ineffective, pharma]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID                                               text  \\\n",
       "0  1311981051720409089t  @sandraburgess3 They have no idea , they cant ...   \n",
       "1  1361403925845401601t  @stepheniscowboy Nvm I â€™ ve had covid I â€™ ve g...   \n",
       "2  1293488278361055233t  Coronavirus updates : Government partners with...   \n",
       "3  1305252218526990338t  @OANN U . K . Glaxo Smith Klein whistleblower ...   \n",
       "4  1376135683400687618t  3 / horse \" AstraZeneca , not so much for the ...   \n",
       "\n",
       "               labels            labels_list  \\\n",
       "0         ineffective          [ineffective]   \n",
       "1         unnecessary          [unnecessary]   \n",
       "2              pharma               [pharma]   \n",
       "3              rushed               [rushed]   \n",
       "4  ineffective pharma  [ineffective, pharma]   \n",
       "\n",
       "                         labels_encoded  conspiracy  country  ineffective  \\\n",
       "0  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]           0        0            1   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]           0        0            0   \n",
       "2  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]           0        0            0   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]           0        0            0   \n",
       "4  [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]           0        0            1   \n",
       "\n",
       "   ingredients  mandatory  none  pharma  political  religious  rushed  \\\n",
       "0            0          0     0       0          0          0       0   \n",
       "1            0          0     0       0          0          0       0   \n",
       "2            0          0     0       1          0          0       0   \n",
       "3            0          0     0       0          0          0       1   \n",
       "4            0          0     0       1          0          0       0   \n",
       "\n",
       "   side-effect  unnecessary  \n",
       "0            0            0  \n",
       "1            0            1  \n",
       "2            0            0  \n",
       "3            0            0  \n",
       "4            0            0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Create Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 19:58:56.327094: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 19:58:56.354998: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 19:58:56.355179: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 19:58:56.356322: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 19:58:56.356489: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 19:58:56.356639: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 19:58:56.417752: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 19:58:56.417911: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 19:58:56.418057: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-19 19:58:56.418165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22118 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:08:00.0, compute capability: 8.6\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBartForSequenceClassification: ['model.decoder.version', 'model.encoder.version']\n",
      "- This IS expected if you are initializing TFBartForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBartForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# the model that will be used for classification\n",
    "model_name = 'facebook/bart-large-mnli'\n",
    "\n",
    "# create the classifier\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Test Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels_encoded': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
      " 'labels_list': ['rushed', 'side-effect'],\n",
      " 'text': '@eleonorasfalcon @Phoebejoy1611 And what about the long term effects '\n",
      "         \"of an untrialled vaccine ? You can stop taking pills but you can't \"\n",
      "         'take a vaccine out of your body .'}\n"
     ]
    }
   ],
   "source": [
    "# select a row for testing\n",
    "sample_row = df_train.iloc[146][['text', 'labels_list', 'labels_encoded']]\n",
    "pprint(sample_row.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': ['side-effect',\n",
      "            'ineffective',\n",
      "            'rushed',\n",
      "            'unnecessary',\n",
      "            'conspiracy',\n",
      "            'country',\n",
      "            'pharma',\n",
      "            'political',\n",
      "            'mandatory',\n",
      "            'ingredients',\n",
      "            'religious',\n",
      "            'none'],\n",
      " 'scores': [0.9871069192886353,\n",
      "            0.7577099204063416,\n",
      "            0.5949831008911133,\n",
      "            0.4207984209060669,\n",
      "            0.09220512211322784,\n",
      "            0.07710351794958115,\n",
      "            0.07344336807727814,\n",
      "            0.028462089598178864,\n",
      "            0.0051172408275306225,\n",
      "            0.002520242240279913,\n",
      "            0.0013842779444530606,\n",
      "            0.0006486524362117052],\n",
      " 'sequence': '@eleonorasfalcon @Phoebejoy1611 And what about the long term '\n",
      "             'effects of an untrialled vaccine ? You can stop taking pills but '\n",
      "             \"you can't take a vaccine out of your body .\"}\n"
     ]
    }
   ],
   "source": [
    "# perform classification\n",
    "result = classifier(\n",
    "    sequences=sample_row.text,\n",
    "    candidate_labels=vocab,\n",
    "    hypothesis_template='This concern with the vaccine is about {}.',\n",
    "    multi_label=True)\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Get Standardized Predictions\n",
    "\n",
    "Standardize the prediction to match the order of the labels in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_prediction(prediction: Dict, vocabulary:List[str]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Standardize the prediction output to a fixed length list.\n",
    "    \"\"\"\n",
    "    return [prediction['scores'][prediction['labels'].index(label)]\n",
    "            for label in vocabulary]\n",
    "\n",
    "## test the function\n",
    "#standardize_prediction(result, vocab.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(text:str, classifier, vocabulary:List[str]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Get the prediction for a given text.\n",
    "    \"\"\"\n",
    "    result = classifier(\n",
    "        sequences=text,\n",
    "        candidate_labels=vocabulary,\n",
    "        hypothesis_template='This concern with the vaccine is about {}.',\n",
    "        multi_label=True)\n",
    "    \n",
    "    return standardize_prediction(result, vocabulary)\n",
    "\n",
    "## test the function\n",
    "#get_prediction(sample_row.text, classifier, vocab.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: List[str], vocabulary:List[str], classifier, n_jobs:int=1) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Predict the labels for a list of texts.\n",
    "    \"\"\"\n",
    "    if n_jobs == 1:\n",
    "        result = []\n",
    "        for text in tqdm(X):\n",
    "            result.append(get_prediction(text, classifier, vocabulary))\n",
    "            \n",
    "        return result\n",
    "    else:\n",
    "        # create the partial function for parallel processing\n",
    "        get_prediction_partial = partial(get_prediction, classifier=classifier, vocabulary=vocabulary)\n",
    "    \n",
    "        # perform parallel processing \n",
    "        return pqdm(X, get_prediction_partial, n_jobs=5)\n",
    "        \n",
    "## test the function\n",
    "# predict(\n",
    "#     X=df_train[:5].text.tolist(), \n",
    "#     vocabulary=vocab.tolist(),\n",
    "#     classifier=classifier,\n",
    "#     n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_score_macro(y_true, y_pred):\n",
    "        \"\"\"Calculate F1-score (Macro-Average).\"\"\"\n",
    "        return f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_score_weighted(y_true, y_pred):\n",
    "        \"\"\"Calculate F1-score (Weighted-Average).\"\"\"\n",
    "        return f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def jaccard_similarity(y_true, y_pred):\n",
    "        \"\"\"Calculate average Jaccard Similarity.\"\"\"\n",
    "        return jaccard_score(y_true, y_pred, average='samples')\n",
    "\n",
    "    @staticmethod\n",
    "    def subset_accuracy(y_true, y_pred):\n",
    "        \"\"\"Calculate Subset Accuracy (Exact Match Accuracy).\"\"\"\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_all(y_true,\n",
    "                     y_pred,\n",
    "                     threshold:float=0.5):\n",
    "        \n",
    "        # Convert predictions to binary\n",
    "        y_pred_bin = [[int(prob > threshold) for prob in pred] for pred in y_pred]\n",
    "        \n",
    "        \"\"\"Evaluate all metrics and display a summary.\"\"\"\n",
    "        f1_macro = Evaluation.f1_score_macro(y_true, y_pred_bin)\n",
    "        f1_weighted = Evaluation.f1_score_weighted(y_true, y_pred_bin)\n",
    "        jaccard_similarity = Evaluation.jaccard_similarity(y_true, y_pred_bin)\n",
    "        subset_accuracy = Evaluation.subset_accuracy(y_true, y_pred_bin)\n",
    "\n",
    "        # Display a summary of the evaluation\n",
    "        print(f\"F1 Score (Macro-Average)   \\t{f1_macro:.3f}\")\n",
    "        print(f\"F1 Score (Weighted-Average)\\t{f1_weighted:.3f}\")\n",
    "        print(f\"Average Jaccard Similarity \\t{jaccard_similarity:.3f}\")\n",
    "        print(f\"Subset Accuracy            \\t{subset_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Perform Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9488a9651044ce3b6c20827be0310ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/6957 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f689e69bece74d85875dd0fd5e6423c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/6957 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6722a4dcf9774239ab9cf9989a8cb408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/6957 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = df_train\n",
    "\n",
    "# get the true values\n",
    "y_true = data[vocab].values.tolist()\n",
    "\n",
    "# get the predictions\n",
    "y_pred = predict(X=data.text.tolist(),\n",
    "        vocabulary=vocab.tolist(),\n",
    "        classifier=classifier,\n",
    "        n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predictions to a file\n",
    "np.save(f'{output_path}/02-01_bart-large-mnli_train.npy', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_read = np.load(f'{output_path}/02-01_bart-large-mnli_train.npy', allow_pickle=True)\n",
    "# temp_read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_classification_report(data:pd.DataFrame,\n",
    "                               y_pred:np.ndarray,\n",
    "                               threshold:float=0.5):\n",
    "    # get the true labels\n",
    "    y_true = data[vocab].values\n",
    "    \n",
    "    # Convert predictions to binary\n",
    "    y_pred_bin = [[int(prob > threshold) for prob in pred] for pred in y_pred]\n",
    "    \n",
    "    # show the classification report\n",
    "    print(classification_report(y_true, y_pred_bin, target_names=vocab))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  conspiracy       0.12      0.70      0.20       341\n",
      "     country       0.03      0.59      0.06       140\n",
      " ineffective       0.42      0.69      0.52      1171\n",
      " ingredients       0.55      0.47      0.51       304\n",
      "   mandatory       0.40      0.59      0.48       548\n",
      "        none       0.10      0.06      0.08       440\n",
      "      pharma       0.30      0.74      0.42       889\n",
      "   political       0.17      0.85      0.28       437\n",
      "   religious       0.25      0.64      0.36        45\n",
      "      rushed       0.22      0.88      0.35      1032\n",
      " side-effect       0.59      0.83      0.69      2663\n",
      " unnecessary       0.12      0.87      0.22       503\n",
      "\n",
      "   micro avg       0.26      0.73      0.39      8513\n",
      "   macro avg       0.27      0.66      0.35      8513\n",
      "weighted avg       0.37      0.73      0.46      8513\n",
      " samples avg       0.30      0.73      0.40      8513\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnny/swan/miniconda3/envs/caves-data/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# show the test classification report\n",
    "show_classification_report(data, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Full Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Macro-Average)   \t0.347\n",
      "F1 Score (Weighted-Average)\t0.464\n",
      "Average Jaccard Similarity \t0.293\n",
      "Subset Accuracy            \t0.068\n"
     ]
    }
   ],
   "source": [
    "Evaluation.evaluate_all(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caves-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
